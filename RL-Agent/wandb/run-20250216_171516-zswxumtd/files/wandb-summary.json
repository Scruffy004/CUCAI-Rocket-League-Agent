{"_step":204356,"Policy Entropy":4.105396588643392,"Timesteps Collected":50026,"Value Function Update Magnitude":0.31756970286369324,"PPO Batch Consumption Time":0.27863383293151855,"x_vel":34.49525549033699,"Policy Reward":50.178489881959855,"Value Function Loss":0.004791710680971543,"Total Iteration Time":5.181212700001197,"Policy Update Magnitude":0.25335195660591125,"Cumulative Model Updates":331638,"Mean KL Divergence":0.003055680931235353,"Timestep Consumption Time":2.445428900000479,"_timestamp":1.7397558787775977e+09,"Cumulative Timesteps":2765822762,"Timestep Collection Time":2.7357838000007177,"_runtime":489903.6773158,"Collected Steps per Second":18285.801677744737,"z_vel":8.003651642468286,"y_vel":64.21723377850935,"_wandb":{"runtime":489903},"SB3 Clip Fraction":0.022116666038831074,"Overall Steps per Second":9655.268543595681}