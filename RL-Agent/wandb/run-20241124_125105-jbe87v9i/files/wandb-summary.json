{"Policy Entropy":0.7049321234226227,"Timestep Consumption Time":1.391760399999999,"Cumulative Model Updates":75,"Value Function Loss":0.023645582608878613,"y_vel":430.8404152119543,"Timesteps Collected":50026,"SB3 Clip Fraction":0,"z_vel":19.489354036483157,"_wandb":{"runtime":311},"Total Iteration Time":3.7451220999999997,"Policy Update Magnitude":0.03514060750603676,"Collected Steps per Second":21257.25085098478,"PPO Batch Consumption Time":0.3320884704589844,"_runtime":311.6276952,"Timestep Collection Time":2.3533617000000007,"x_vel":10.305437255170267,"_step":157,"_timestamp":1.732470673501969e+09,"Value Function Update Magnitude":0.038272906094789505,"Policy Reward":-0.038306105080632466,"Overall Steps per Second":13357.64193108684,"Mean KL Divergence":0.00017520602341392078,"Cumulative Timesteps":1350690}